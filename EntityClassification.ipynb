{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from sklearn.datasets import load_files \n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.attrs import ORTH\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = load_files(r\"/Users/karthekeyanmani/Downloads/bbc\")  \n",
    "X, y = list(map(str,rawData.data)), to_categorical(np.asarray(list(map(int,rawData.target))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['PERSON',\n",
    " 'NORP',\n",
    " 'FAC',\n",
    " 'ORG',\n",
    " 'GPE',\n",
    " 'LOC',\n",
    " 'PRODUCT',\n",
    " 'EVENT',\n",
    " 'WORK_OF_ART',\n",
    " 'LAW',\n",
    " 'LANGUAGE',\n",
    " 'DATE',\n",
    " 'TIME',\n",
    " 'PERCENT',\n",
    " 'MONEY',\n",
    " 'QUANTITY',\n",
    " 'ORDINAL',\n",
    " 'CARDINAL',\n",
    " 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictRow = {'PERSON':0,\n",
    "           'NORP':0,\n",
    "           'FAC':0,\n",
    "           'ORG':0,\n",
    "           'GPE':0,\n",
    "           'LOC':0,\n",
    "           'PRODUCT':0,\n",
    "           'EVENT':0,\n",
    "           'WORK_OF_ART':0,\n",
    "           'LAW':0,\n",
    "           'LANGUAGE':0,\n",
    "           'DATE':0,\n",
    "           'TIME':0,\n",
    "           'PERCENT':0,\n",
    "           'MONEY':0,\n",
    "           'QUANTITY':0,\n",
    "           'ORDINAL':0,\n",
    "           'CARDINAL':0,\n",
    "           'Label':''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEnt = pd.DataFrame([],columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for i in range(len(X)):\n",
    "    doc = nlp(X[i])\n",
    "    dictRow = {'PERSON':0,\n",
    "           'NORP':0,\n",
    "           'FAC':0,\n",
    "           'ORG':0,\n",
    "           'GPE':0,\n",
    "           'LOC':0,\n",
    "           'PRODUCT':0,\n",
    "           'EVENT':0,\n",
    "           'WORK_OF_ART':0,\n",
    "           'LAW':0,\n",
    "           'LANGUAGE':0,\n",
    "           'DATE':0,\n",
    "           'TIME':0,\n",
    "           'PERCENT':0,\n",
    "           'MONEY':0,\n",
    "           'QUANTITY':0,\n",
    "           'ORDINAL':0,\n",
    "           'CARDINAL':0,\n",
    "           'Label':y[i]}\n",
    "    for ent in doc.ents:\n",
    "        dictRow[ent.label_] = dictRow[ent.label_]+1\n",
    "    dfEnt = dfEnt.append(dictRow,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthekeyanmani/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "FEATURES = dfEnt.columns[0:len(dfEnt.columns)-1]\n",
    "X_data = dfEnt[FEATURES].as_matrix()\n",
    "X_data = normalize(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1557, 18)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "Train on 1401 samples, validate on 156 samples\n",
      "Epoch 1/100\n",
      "1401/1401 [==============================] - 1s 942us/sample - loss: 1.3992 - acc: 0.4354 - val_loss: 1.2382 - val_acc: 0.5064\n",
      "Epoch 2/100\n",
      "1401/1401 [==============================] - 1s 431us/sample - loss: 1.1463 - acc: 0.5603 - val_loss: 1.0794 - val_acc: 0.6026\n",
      "Epoch 3/100\n",
      "1401/1401 [==============================] - 1s 443us/sample - loss: 1.0566 - acc: 0.5931 - val_loss: 1.0311 - val_acc: 0.6282\n",
      "Epoch 4/100\n",
      "1401/1401 [==============================] - 1s 448us/sample - loss: 1.0207 - acc: 0.6103 - val_loss: 0.9914 - val_acc: 0.6410\n",
      "Epoch 5/100\n",
      "1401/1401 [==============================] - 1s 444us/sample - loss: 0.9907 - acc: 0.6110 - val_loss: 0.9768 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "1401/1401 [==============================] - 1s 442us/sample - loss: 0.9763 - acc: 0.6431 - val_loss: 0.9661 - val_acc: 0.6346\n",
      "Epoch 7/100\n",
      "1401/1401 [==============================] - 1s 443us/sample - loss: 0.9603 - acc: 0.6231 - val_loss: 0.9739 - val_acc: 0.6282\n",
      "Epoch 8/100\n",
      "1401/1401 [==============================] - 1s 453us/sample - loss: 0.9443 - acc: 0.6381 - val_loss: 0.9591 - val_acc: 0.6474\n",
      "Epoch 9/100\n",
      "1401/1401 [==============================] - 1s 444us/sample - loss: 0.9383 - acc: 0.6360 - val_loss: 0.9909 - val_acc: 0.6026\n",
      "Epoch 10/100\n",
      "1401/1401 [==============================] - 1s 442us/sample - loss: 0.9454 - acc: 0.6331 - val_loss: 0.9853 - val_acc: 0.6090\n",
      "Epoch 11/100\n",
      "1401/1401 [==============================] - 1s 456us/sample - loss: 0.9288 - acc: 0.6424 - val_loss: 0.9512 - val_acc: 0.6346\n",
      "Epoch 12/100\n",
      "1401/1401 [==============================] - 1s 450us/sample - loss: 0.9158 - acc: 0.6517 - val_loss: 0.9472 - val_acc: 0.6410\n",
      "Epoch 13/100\n",
      "1401/1401 [==============================] - 1s 493us/sample - loss: 0.9131 - acc: 0.6388 - val_loss: 0.9621 - val_acc: 0.6090\n",
      "Epoch 14/100\n",
      "1401/1401 [==============================] - 1s 445us/sample - loss: 0.9067 - acc: 0.6510 - val_loss: 0.9494 - val_acc: 0.6474\n",
      "Epoch 15/100\n",
      "1401/1401 [==============================] - 1s 439us/sample - loss: 0.9032 - acc: 0.6445 - val_loss: 0.9511 - val_acc: 0.6282\n",
      "Epoch 16/100\n",
      "1401/1401 [==============================] - 1s 443us/sample - loss: 0.9067 - acc: 0.6517 - val_loss: 0.9499 - val_acc: 0.6026\n",
      "Epoch 17/100\n",
      "1401/1401 [==============================] - 1s 448us/sample - loss: 0.8995 - acc: 0.6488 - val_loss: 0.9516 - val_acc: 0.6218\n",
      "Epoch 18/100\n",
      "1401/1401 [==============================] - 1s 444us/sample - loss: 0.8923 - acc: 0.6495 - val_loss: 0.9614 - val_acc: 0.6026\n",
      "Epoch 19/100\n",
      "1401/1401 [==============================] - 1s 440us/sample - loss: 0.8870 - acc: 0.6524 - val_loss: 0.9366 - val_acc: 0.6410\n",
      "Epoch 20/100\n",
      "1401/1401 [==============================] - 1s 458us/sample - loss: 0.8836 - acc: 0.6567 - val_loss: 0.9577 - val_acc: 0.6218\n",
      "Epoch 21/100\n",
      "1401/1401 [==============================] - 1s 451us/sample - loss: 0.8764 - acc: 0.6645 - val_loss: 0.9518 - val_acc: 0.6218\n",
      "Epoch 22/100\n",
      "1401/1401 [==============================] - 1s 443us/sample - loss: 0.8763 - acc: 0.6759 - val_loss: 0.9605 - val_acc: 0.6154\n",
      "Epoch 23/100\n",
      "1401/1401 [==============================] - 1s 462us/sample - loss: 0.8619 - acc: 0.6595 - val_loss: 0.9664 - val_acc: 0.6026\n",
      "Epoch 24/100\n",
      "1401/1401 [==============================] - 1s 453us/sample - loss: 0.8623 - acc: 0.6738 - val_loss: 0.9394 - val_acc: 0.6474\n",
      "Epoch 25/100\n",
      "1401/1401 [==============================] - 1s 446us/sample - loss: 0.8522 - acc: 0.6645 - val_loss: 0.9428 - val_acc: 0.6538\n",
      "Epoch 26/100\n",
      "1401/1401 [==============================] - 1s 461us/sample - loss: 0.8486 - acc: 0.6781 - val_loss: 0.9490 - val_acc: 0.6282\n",
      "Epoch 27/100\n",
      "1401/1401 [==============================] - 1s 469us/sample - loss: 0.8576 - acc: 0.6652 - val_loss: 0.9813 - val_acc: 0.6026\n",
      "Epoch 28/100\n",
      "1401/1401 [==============================] - 1s 459us/sample - loss: 0.8488 - acc: 0.6667 - val_loss: 0.9566 - val_acc: 0.6218\n",
      "Epoch 29/100\n",
      "1401/1401 [==============================] - 1s 468us/sample - loss: 0.8416 - acc: 0.6745 - val_loss: 0.9381 - val_acc: 0.6603\n",
      "Epoch 30/100\n",
      "1401/1401 [==============================] - 1s 739us/sample - loss: 0.8385 - acc: 0.6738 - val_loss: 0.9424 - val_acc: 0.6538\n",
      "Epoch 31/100\n",
      "1401/1401 [==============================] - 1s 585us/sample - loss: 0.8329 - acc: 0.6838 - val_loss: 0.9551 - val_acc: 0.6154\n",
      "Epoch 32/100\n",
      "1401/1401 [==============================] - 1s 456us/sample - loss: 0.8381 - acc: 0.6759 - val_loss: 0.9589 - val_acc: 0.6282\n",
      "Epoch 33/100\n",
      "1401/1401 [==============================] - 1s 455us/sample - loss: 0.8212 - acc: 0.6845 - val_loss: 0.9814 - val_acc: 0.6154\n",
      "Epoch 34/100\n",
      "1401/1401 [==============================] - 1s 444us/sample - loss: 0.8308 - acc: 0.6824 - val_loss: 0.9503 - val_acc: 0.6410\n",
      "Epoch 35/100\n",
      "1401/1401 [==============================] - 1s 439us/sample - loss: 0.8214 - acc: 0.6795 - val_loss: 0.9685 - val_acc: 0.6154\n",
      "Epoch 36/100\n",
      "1401/1401 [==============================] - 1s 465us/sample - loss: 0.8137 - acc: 0.6924 - val_loss: 0.9727 - val_acc: 0.6090\n",
      "Epoch 37/100\n",
      "1401/1401 [==============================] - 1s 475us/sample - loss: 0.8065 - acc: 0.6809 - val_loss: 0.9694 - val_acc: 0.6410\n",
      "Epoch 38/100\n",
      "1401/1401 [==============================] - 1s 556us/sample - loss: 0.8128 - acc: 0.6845 - val_loss: 0.9646 - val_acc: 0.6346\n",
      "Epoch 39/100\n",
      "1401/1401 [==============================] - 1s 455us/sample - loss: 0.8091 - acc: 0.6931 - val_loss: 0.9714 - val_acc: 0.6218\n",
      "Epoch 40/100\n",
      "1401/1401 [==============================] - 1s 444us/sample - loss: 0.7909 - acc: 0.6845 - val_loss: 0.9593 - val_acc: 0.6154\n",
      "Epoch 41/100\n",
      "1401/1401 [==============================] - 1s 463us/sample - loss: 0.8109 - acc: 0.6838 - val_loss: 0.9755 - val_acc: 0.6282\n",
      "Epoch 42/100\n",
      "1401/1401 [==============================] - 1s 450us/sample - loss: 0.8053 - acc: 0.6874 - val_loss: 0.9670 - val_acc: 0.6282\n",
      "Epoch 43/100\n",
      "1401/1401 [==============================] - 1s 449us/sample - loss: 0.8017 - acc: 0.6817 - val_loss: 0.9850 - val_acc: 0.6090\n",
      "Epoch 44/100\n",
      "1401/1401 [==============================] - 1s 441us/sample - loss: 0.7926 - acc: 0.7031 - val_loss: 0.9804 - val_acc: 0.6154\n",
      "Epoch 45/100\n",
      "1401/1401 [==============================] - 1s 460us/sample - loss: 0.7925 - acc: 0.6952 - val_loss: 0.9930 - val_acc: 0.6026\n",
      "Epoch 46/100\n",
      "1401/1401 [==============================] - 1s 444us/sample - loss: 0.7812 - acc: 0.7045 - val_loss: 0.9858 - val_acc: 0.6282\n",
      "Epoch 47/100\n",
      "1401/1401 [==============================] - 1s 440us/sample - loss: 0.7746 - acc: 0.6966 - val_loss: 0.9907 - val_acc: 0.5962\n",
      "Epoch 48/100\n",
      "1401/1401 [==============================] - 1s 436us/sample - loss: 0.7755 - acc: 0.7024 - val_loss: 0.9696 - val_acc: 0.5962\n",
      "Epoch 49/100\n",
      "1401/1401 [==============================] - 1s 452us/sample - loss: 0.7675 - acc: 0.6981 - val_loss: 0.9805 - val_acc: 0.6346\n",
      "Epoch 50/100\n",
      "1401/1401 [==============================] - 1s 447us/sample - loss: 0.7731 - acc: 0.7016 - val_loss: 0.9716 - val_acc: 0.6282\n",
      "Epoch 51/100\n",
      "1401/1401 [==============================] - 1s 471us/sample - loss: 0.7724 - acc: 0.6916 - val_loss: 1.0046 - val_acc: 0.6154\n",
      "Epoch 52/100\n",
      "1401/1401 [==============================] - 1s 483us/sample - loss: 0.7750 - acc: 0.7074 - val_loss: 0.9989 - val_acc: 0.6282\n",
      "Epoch 53/100\n",
      "1401/1401 [==============================] - 1s 461us/sample - loss: 0.7617 - acc: 0.7031 - val_loss: 0.9884 - val_acc: 0.6218\n",
      "Epoch 54/100\n",
      "1401/1401 [==============================] - 1s 442us/sample - loss: 0.7647 - acc: 0.6966 - val_loss: 1.0075 - val_acc: 0.5897\n",
      "Epoch 55/100\n",
      "1401/1401 [==============================] - 1s 441us/sample - loss: 0.7723 - acc: 0.7002 - val_loss: 1.0068 - val_acc: 0.6090\n",
      "Epoch 56/100\n",
      "1401/1401 [==============================] - 1s 436us/sample - loss: 0.7622 - acc: 0.7066 - val_loss: 0.9967 - val_acc: 0.6090\n",
      "Epoch 57/100\n",
      "1401/1401 [==============================] - 1s 456us/sample - loss: 0.7481 - acc: 0.7066 - val_loss: 1.0131 - val_acc: 0.5962\n",
      "Epoch 58/100\n",
      "1401/1401 [==============================] - 1s 446us/sample - loss: 0.7492 - acc: 0.7123 - val_loss: 1.0090 - val_acc: 0.6218\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401/1401 [==============================] - 1s 435us/sample - loss: 0.7364 - acc: 0.7159 - val_loss: 1.0183 - val_acc: 0.6090\n",
      "Epoch 60/100\n",
      "1401/1401 [==============================] - 1s 431us/sample - loss: 0.7392 - acc: 0.7145 - val_loss: 1.0038 - val_acc: 0.6154\n",
      "Epoch 61/100\n",
      "1401/1401 [==============================] - 1s 426us/sample - loss: 0.7338 - acc: 0.7159 - val_loss: 1.0060 - val_acc: 0.6218\n",
      "Epoch 62/100\n",
      "1401/1401 [==============================] - 1s 426us/sample - loss: 0.7379 - acc: 0.7088 - val_loss: 1.0094 - val_acc: 0.6090\n",
      "Epoch 63/100\n",
      "1401/1401 [==============================] - 1s 428us/sample - loss: 0.7475 - acc: 0.7059 - val_loss: 1.0137 - val_acc: 0.6218\n",
      "Epoch 64/100\n",
      "1401/1401 [==============================] - 1s 430us/sample - loss: 0.7204 - acc: 0.7202 - val_loss: 1.0235 - val_acc: 0.6026\n",
      "Epoch 65/100\n",
      "1401/1401 [==============================] - 1s 425us/sample - loss: 0.7427 - acc: 0.7116 - val_loss: 1.0308 - val_acc: 0.6090\n",
      "Epoch 66/100\n",
      "1401/1401 [==============================] - 1s 427us/sample - loss: 0.7239 - acc: 0.7181 - val_loss: 1.0239 - val_acc: 0.6218\n",
      "Epoch 67/100\n",
      "1401/1401 [==============================] - 1s 432us/sample - loss: 0.7216 - acc: 0.7195 - val_loss: 1.0215 - val_acc: 0.6154\n",
      "Epoch 68/100\n",
      "1401/1401 [==============================] - 1s 432us/sample - loss: 0.7247 - acc: 0.7116 - val_loss: 1.0277 - val_acc: 0.6026\n",
      "Epoch 69/100\n",
      "1401/1401 [==============================] - 1s 429us/sample - loss: 0.7271 - acc: 0.7216 - val_loss: 1.0179 - val_acc: 0.6090\n",
      "Epoch 70/100\n",
      "1401/1401 [==============================] - 1s 432us/sample - loss: 0.7155 - acc: 0.7323 - val_loss: 1.0360 - val_acc: 0.5962\n",
      "Epoch 71/100\n",
      "1401/1401 [==============================] - 1s 443us/sample - loss: 0.7168 - acc: 0.7145 - val_loss: 1.0431 - val_acc: 0.6218\n",
      "Epoch 72/100\n",
      "1401/1401 [==============================] - 1s 439us/sample - loss: 0.7195 - acc: 0.7288 - val_loss: 1.0462 - val_acc: 0.5962\n",
      "Epoch 73/100\n",
      "1401/1401 [==============================] - 1s 436us/sample - loss: 0.7149 - acc: 0.7238 - val_loss: 1.0678 - val_acc: 0.6026\n",
      "Epoch 74/100\n",
      "1401/1401 [==============================] - 1s 433us/sample - loss: 0.7091 - acc: 0.7231 - val_loss: 1.0611 - val_acc: 0.6346\n",
      "Epoch 75/100\n",
      "1401/1401 [==============================] - 1s 432us/sample - loss: 0.7052 - acc: 0.7388 - val_loss: 1.0624 - val_acc: 0.6218\n",
      "Epoch 76/100\n",
      "1401/1401 [==============================] - 1s 432us/sample - loss: 0.7034 - acc: 0.7359 - val_loss: 1.0966 - val_acc: 0.5833\n",
      "Epoch 77/100\n",
      "1401/1401 [==============================] - 1s 433us/sample - loss: 0.7078 - acc: 0.7245 - val_loss: 1.0667 - val_acc: 0.6154\n",
      "Epoch 78/100\n",
      "1401/1401 [==============================] - 1s 428us/sample - loss: 0.7027 - acc: 0.7323 - val_loss: 1.0723 - val_acc: 0.5833\n",
      "Epoch 79/100\n",
      "1401/1401 [==============================] - 1s 428us/sample - loss: 0.7034 - acc: 0.7266 - val_loss: 1.0840 - val_acc: 0.6090\n",
      "Epoch 80/100\n",
      "1401/1401 [==============================] - 1s 434us/sample - loss: 0.6999 - acc: 0.7273 - val_loss: 1.0733 - val_acc: 0.6218\n",
      "Epoch 81/100\n",
      "1401/1401 [==============================] - 1s 433us/sample - loss: 0.6787 - acc: 0.7316 - val_loss: 1.0885 - val_acc: 0.5962\n",
      "Epoch 82/100\n",
      "1401/1401 [==============================] - 1s 443us/sample - loss: 0.6930 - acc: 0.7395 - val_loss: 1.0719 - val_acc: 0.5897\n",
      "Epoch 83/100\n",
      "1401/1401 [==============================] - 1s 441us/sample - loss: 0.6793 - acc: 0.7409 - val_loss: 1.0790 - val_acc: 0.5705\n",
      "Epoch 84/100\n",
      "1401/1401 [==============================] - 1s 437us/sample - loss: 0.6968 - acc: 0.7288 - val_loss: 1.0792 - val_acc: 0.6090\n",
      "Epoch 85/100\n",
      "1401/1401 [==============================] - 1s 433us/sample - loss: 0.6912 - acc: 0.7273 - val_loss: 1.1026 - val_acc: 0.5962\n",
      "Epoch 86/100\n",
      "1401/1401 [==============================] - 1s 434us/sample - loss: 0.6810 - acc: 0.7395 - val_loss: 1.0999 - val_acc: 0.5897\n",
      "Epoch 87/100\n",
      "1401/1401 [==============================] - 1s 431us/sample - loss: 0.6720 - acc: 0.7452 - val_loss: 1.0966 - val_acc: 0.6026\n",
      "Epoch 88/100\n",
      "1401/1401 [==============================] - 1s 431us/sample - loss: 0.6833 - acc: 0.7373 - val_loss: 1.0832 - val_acc: 0.5705\n",
      "Epoch 89/100\n",
      "1401/1401 [==============================] - 1s 441us/sample - loss: 0.6660 - acc: 0.7466 - val_loss: 1.0964 - val_acc: 0.5897\n",
      "Epoch 90/100\n",
      "1401/1401 [==============================] - 1s 438us/sample - loss: 0.6643 - acc: 0.7495 - val_loss: 1.0841 - val_acc: 0.6154\n",
      "Epoch 91/100\n",
      "1401/1401 [==============================] - 1s 443us/sample - loss: 0.6745 - acc: 0.7388 - val_loss: 1.0822 - val_acc: 0.6154\n",
      "Epoch 92/100\n",
      "1401/1401 [==============================] - 1s 437us/sample - loss: 0.6584 - acc: 0.7445 - val_loss: 1.0977 - val_acc: 0.5962\n",
      "Epoch 93/100\n",
      "1401/1401 [==============================] - 1s 441us/sample - loss: 0.6851 - acc: 0.7373 - val_loss: 1.0879 - val_acc: 0.5769\n",
      "Epoch 94/100\n",
      "1401/1401 [==============================] - 1s 432us/sample - loss: 0.6540 - acc: 0.7395 - val_loss: 1.0983 - val_acc: 0.5833\n",
      "Epoch 95/100\n",
      "1401/1401 [==============================] - 1s 436us/sample - loss: 0.6444 - acc: 0.7495 - val_loss: 1.1225 - val_acc: 0.5962\n",
      "Epoch 96/100\n",
      "1401/1401 [==============================] - 1s 448us/sample - loss: 0.6659 - acc: 0.7423 - val_loss: 1.1164 - val_acc: 0.5705\n",
      "Epoch 97/100\n",
      "1401/1401 [==============================] - 1s 500us/sample - loss: 0.6658 - acc: 0.7473 - val_loss: 1.1035 - val_acc: 0.6090\n",
      "Epoch 98/100\n",
      "1401/1401 [==============================] - 1s 513us/sample - loss: 0.6561 - acc: 0.7573 - val_loss: 1.1145 - val_acc: 0.5513\n",
      "Epoch 99/100\n",
      "1401/1401 [==============================] - 1s 483us/sample - loss: 0.6537 - acc: 0.7430 - val_loss: 1.1126 - val_acc: 0.5769\n",
      "Epoch 100/100\n",
      "1401/1401 [==============================] - 1s 464us/sample - loss: 0.6457 - acc: 0.7537 - val_loss: 1.1205 - val_acc: 0.6090\n",
      " - 0s - loss: 1.0595 - acc: 0.6362\n",
      "Test loss: 1.0594531577638167\n",
      "Test accuracy: 0.63622755\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.save_weights\n",
    "model.add(Dense(512, input_shape=(len(FEATURES),)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(rawData.target_names)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.metrics_names)\n",
    "batch_size = 5\n",
    "epochs = 100\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1,callbacks=[tensorboard])\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=2)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestData(text):\n",
    "    doc = nlp(text)\n",
    "    cols = ['PERSON',\n",
    "             'NORP',\n",
    "             'FAC',\n",
    "             'ORG',\n",
    "             'GPE',\n",
    "             'LOC',\n",
    "             'PRODUCT',\n",
    "             'EVENT',\n",
    "             'WORK_OF_ART',\n",
    "             'LAW',\n",
    "             'LANGUAGE',\n",
    "             'DATE',\n",
    "             'TIME',\n",
    "             'PERCENT',\n",
    "             'MONEY',\n",
    "             'QUANTITY',\n",
    "             'ORDINAL',\n",
    "             'CARDINAL',\n",
    "             'Label']\n",
    "    dfTemp = pd.DataFrame([],columns=cols)\n",
    "    dictRow = {'PERSON':0,\n",
    "               'NORP':0,\n",
    "               'FAC':0,\n",
    "               'ORG':0,\n",
    "               'GPE':0,\n",
    "               'LOC':0,\n",
    "               'PRODUCT':0,\n",
    "               'EVENT':0,\n",
    "               'WORK_OF_ART':0,\n",
    "               'LAW':0,\n",
    "               'LANGUAGE':0,\n",
    "               'DATE':0,\n",
    "               'TIME':0,\n",
    "               'PERCENT':0,\n",
    "               'MONEY':0,\n",
    "               'QUANTITY':0,\n",
    "               'ORDINAL':0,\n",
    "               'CARDINAL':0,\n",
    "               'Label':y[i]}\n",
    "    for ent in doc.ents:\n",
    "        dictRow[ent.label_] = dictRow[ent.label_]+1\n",
    "    dfTemp = dfTemp.append(dictRow,ignore_index=True)\n",
    "    FEATURES = dfTemp.columns[0:len(dfTemp.columns)-1]\n",
    "    X_data_temp = dfTemp[FEATURES].as_matrix()\n",
    "    X_data_temp = normalize(X_data_temp)\n",
    "    return X_data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthekeyanmani/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:46: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For prediction \n",
    "text = open(\"test.txt\", \"r\")\n",
    "dtTemp = getTestData(text.read())\n",
    "pre = model.predict_classes(dtTemp)\n",
    "rawData.target_names[pre[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
